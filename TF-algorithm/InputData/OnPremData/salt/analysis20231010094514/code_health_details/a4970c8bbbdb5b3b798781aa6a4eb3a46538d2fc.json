{"last-year-high-resolution-score":9.387218218812514,"high-resolution-score":9.387218218812514,"revision":"46add23cc1","calculated":true,"name":"salt/tests/utils/cptestcase.py","last-month":9,"last-month-details":{"name":"salt/tests/utils/cptestcase.py","score":9,"high-resolution-score":9.387218218812514,"unbiased-score":9.387218218812514,"details":{"cc-mean":9.0,"main-body-cc":0,"lines-in-file":57,"complex-functions":[{"name":"BaseCherryPyTestCase.request","start-line":60,"end-line":126,"cc":9}],"cc-median":9.0,"nested":{"contains-named-nested-functions":true,"nested-complexity-of-interest":[],"max-nested-in-global-scope":0,"max-nested-complexity-depth":0,"max-nested-complexity-depth-name":"<unknown>"},"excess-long-functions":[],"complex-conditionals":[],"longest-fn-loc-name":"BaseCherryPyTestCase.request","n-functions":1,"active-code-size":46,"bumps":{},"longest-fn-loc":46,"cohesion":1,"clone-ratio":0,"fn-args":{"ctors-with-many-args":[],"max-args":9,"n-primitives":0,"fns-with-many-args":[{"name":"BaseCherryPyTestCase.request","start-line":60,"end-line":126,"n-args":9}],"max-ctor-args":0,"n-string-args":0,"mean-args":9.0,"max-ctor-arg-name":"<unknown>","n-args":9,"max-arg-name":"BaseCherryPyTestCase.request"},"cc-max":9,"n-clones":0,"congestion":{"authors":2,"fractal-value":0.44},"median-fn-loc":46.0,"cc-max-name":"BaseCherryPyTestCase.request","cc-total":9},"revision":"46add23cc1","date":"2014-11-22"},"last-year-details":{"details":{"lines-in-file":55},"revision":"177c168a21","date":"2014-08-11"},"unbiased-score":9.387218218812514,"details":{"cc-mean":9.0,"main-body-cc":0,"lines-in-file":57,"social":{"owner":"Seth House","ownership":0.88,"knowledge-loss":0.0},"complex-functions":[{"name":"BaseCherryPyTestCase.request","start-line":60,"end-line":126,"cc":9}],"cc-median":9.0,"nested":{"contains-named-nested-functions":true,"nested-complexity-of-interest":[],"max-nested-in-global-scope":0,"max-nested-complexity-depth":0,"max-nested-complexity-depth-name":"<unknown>"},"code-comment-match":[{"n-matches":0,"pattern-name":"Detect TODOs"}],"excess-long-functions":[],"complex-conditionals":[],"longest-fn-loc-name":"BaseCherryPyTestCase.request","n-functions":1,"active-code-size":46,"bumps":{},"longest-fn-loc":46,"cohesion":1,"delta":{"active-code-delta":0,"cc-total-delta":0,"nested-depth-delta":0,"n-functions-delta":0},"clone-ratio":0,"fn-args":{"ctors-with-many-args":[],"max-args":9,"n-primitives":0,"fns-with-many-args":[{"name":"BaseCherryPyTestCase.request","start-line":60,"end-line":126,"n-args":9}],"max-ctor-args":0,"n-string-args":0,"mean-args":9.0,"max-ctor-arg-name":"<unknown>","n-args":9,"max-arg-name":"BaseCherryPyTestCase.request"},"cc-max":9,"n-clones":0,"congestion":{"authors":0,"fractal-value":0.0},"median-fn-loc":46.0,"cc-max-name":"BaseCherryPyTestCase.request","cc-total":9},"score":9,"last-month-high-resolution-score":9.387218218812514,"file-details":{"name":"salt/tests/utils/cptestcase.py","resolved":"/Users/andreaskarlsson/Documents/Exjobb/algorithm/test-oracle/repos/salt/tests/utils/cptestcase.py","repo-path":"/Users/andreaskarlsson/Documents/Exjobb/algorithm/test-oracle/repos/salt","relative-name":"tests/utils/cptestcase.py"},"last-year":9}
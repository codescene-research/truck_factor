{"last-year-high-resolution-score":8.392646983363955,"high-resolution-score":8.161745981255702,"revision":"b660e7ae","calculated":true,"name":"grape/lib/grape/validations/params_scope.rb","last-month":8,"last-month-details":{"name":"grape/lib/grape/validations/params_scope.rb","score":8,"high-resolution-score":8.161745981255702,"unbiased-score":8.161745981255702,"details":{"cc-mean":4.615384615384615,"main-body-cc":0,"lines-in-file":237,"complex-functions":[{"name":"ParamsScope::infer_coercion","start-line":263,"end-line":288,"cc":11},{"name":"ParamsScope::validates","start-line":204,"end-line":248,"cc":11},{"name":"ParamsScope::should_validate?","start-line":38,"end-line":54,"cc":10}],"cc-median":3.0,"nested":{"contains-named-nested-functions":true,"nested-complexity-of-interest":[],"max-nested-in-global-scope":2,"max-nested-complexity-depth":0,"max-nested-complexity-depth-name":"<unknown>"},"excess-long-functions":[],"complex-conditionals":[{"name":"ParamsScope::should_validate?","details":[{"start-line":39,"end-line":39,"branches":2}]},{"name":"ParamsScope::guess_coerce_type","details":[{"start-line":329,"end-line":329,"branches":2}]}],"longest-fn-loc-name":"ParamsScope::validates","n-functions":26,"active-code-size":226,"bumps":{"fns-with-small-bumps":1,"fns-with-large-bumps":0,"fns-with-severe-bumps":0,"worst-bump":{"bumps":2,"bump-locs":9,"name":"ParamsScope::require_required_and_optional_fields","start-line":114,"end-line":131},"bumps-by-severity":[{"bumps":2,"bump-locs":9,"name":"ParamsScope::require_required_and_optional_fields","start-line":114,"end-line":131}]},"longest-fn-loc":26,"cohesion":1,"clone-ratio":0,"fn-args":{"ctors-with-many-args":[],"max-args":4,"n-primitives":0,"fns-with-many-args":[],"max-ctor-args":0,"n-string-args":0,"mean-args":1.3461538461538463,"max-ctor-arg-name":"<unknown>","n-args":35,"max-arg-name":"ParamsScope::validate"},"cc-max":11,"n-clones":0,"congestion":{"authors":3,"fractal-value":0.67},"median-fn-loc":6.5,"cc-max-name":"ParamsScope::infer_coercion","cc-total":120},"revision":"b660e7ae","date":"2016-07-27"},"last-year-details":{"details":{"lines-in-file":171},"revision":"5154b36e","date":"2015-08-19"},"unbiased-score":8.161745981255702,"details":{"cc-mean":4.615384615384615,"main-body-cc":0,"lines-in-file":237,"social":{"owner":"Oliver Azevedo Barnes","ownership":0.33,"knowledge-loss":0.0},"complex-functions":[{"name":"ParamsScope::infer_coercion","start-line":263,"end-line":288,"cc":11},{"name":"ParamsScope::validates","start-line":204,"end-line":248,"cc":11},{"name":"ParamsScope::should_validate?","start-line":38,"end-line":54,"cc":10}],"cc-median":3.0,"nested":{"contains-named-nested-functions":true,"nested-complexity-of-interest":[],"max-nested-in-global-scope":2,"max-nested-complexity-depth":0,"max-nested-complexity-depth-name":"<unknown>"},"code-comment-match":[{"n-matches":0,"pattern-name":"Detect TODOs"}],"excess-long-functions":[],"complex-conditionals":[{"name":"ParamsScope::should_validate?","details":[{"start-line":39,"end-line":39,"branches":2}]},{"name":"ParamsScope::guess_coerce_type","details":[{"start-line":329,"end-line":329,"branches":2}]}],"longest-fn-loc-name":"ParamsScope::validates","n-functions":26,"active-code-size":226,"bumps":{"fns-with-small-bumps":1,"fns-with-large-bumps":0,"fns-with-severe-bumps":0,"worst-bump":{"bumps":2,"bump-locs":9,"name":"ParamsScope::require_required_and_optional_fields","start-line":114,"end-line":131},"bumps-by-severity":[{"bumps":2,"bump-locs":9,"name":"ParamsScope::require_required_and_optional_fields","start-line":114,"end-line":131}]},"longest-fn-loc":26,"cohesion":1,"delta":{"active-code-delta":0,"cc-total-delta":0,"nested-depth-delta":0,"n-functions-delta":0},"clone-ratio":0,"fn-args":{"ctors-with-many-args":[],"max-args":4,"n-primitives":0,"fns-with-many-args":[],"max-ctor-args":0,"n-string-args":0,"mean-args":1.3461538461538463,"max-ctor-arg-name":"<unknown>","n-args":35,"max-arg-name":"ParamsScope::validate"},"cc-max":11,"n-clones":0,"congestion":{"authors":0,"fractal-value":0.0},"median-fn-loc":6.5,"cc-max-name":"ParamsScope::infer_coercion","cc-total":120},"score":8,"last-month-high-resolution-score":8.161745981255702,"file-details":{"name":"grape/lib/grape/validations/params_scope.rb","resolved":"/Users/andreaskarlsson/Documents/Exjobb/algorithm/test-oracle/repos/grape/lib/grape/validations/params_scope.rb","repo-path":"/Users/andreaskarlsson/Documents/Exjobb/algorithm/test-oracle/repos/grape","relative-name":"lib/grape/validations/params_scope.rb"},"last-year":8}